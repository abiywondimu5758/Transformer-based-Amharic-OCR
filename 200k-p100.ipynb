{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gsutil --upgrade --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-26T15:06:32.058463Z","iopub.execute_input":"2025-06-26T15:06:32.058718Z","iopub.status.idle":"2025-06-26T15:06:54.296171Z","shell.execute_reply.started":"2025-06-26T15:06:32.058696Z","shell.execute_reply":"2025-06-26T15:06:54.295487Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.3/212.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h  Building wheel for gsutil (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for gcs-oauth2-boto-plugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for retry_decorator (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for pyu2f (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.39.0 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngoogle-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-bigtable 2.30.0 requires google-api-core[grpc]<3.0.0,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# import shutil\n\n# shutil.rmtree(\"/kaggle/working/trocr_frozen_head/\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir -p /kaggle/working/mydata\n## !rm -rf /kaggle/working/*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T15:07:26.869961Z","iopub.execute_input":"2025-06-26T15:07:26.870667Z","iopub.status.idle":"2025-06-26T15:07:26.990083Z","shell.execute_reply.started":"2025-06-26T15:07:26.870640Z","shell.execute_reply":"2025-06-26T15:07:26.989075Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip install -U transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T15:07:30.865454Z","iopub.execute_input":"2025-06-26T15:07:30.865802Z","iopub.status.idle":"2025-06-26T15:07:42.816049Z","shell.execute_reply.started":"2025-06-26T15:07:30.865774Z","shell.execute_reply":"2025-06-26T15:07:42.815364Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nCollecting transformers\n  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.51.3\n    Uninstalling transformers-4.51.3:\n      Successfully uninstalled transformers-4.51.3\nSuccessfully installed transformers-4.52.4\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!gsutil -m cp -r gs://adoocr-data/train /kaggle/working/mydata/\n!gsutil -m cp -r gs://adoocr-data/test  /kaggle/working/mydata/ ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T15:07:58.198568Z","iopub.execute_input":"2025-06-26T15:07:58.199167Z","iopub.status.idle":"2025-06-26T15:12:14.309347Z","shell.execute_reply.started":"2025-06-26T15:07:58.199139Z","shell.execute_reply":"2025-06-26T15:12:14.308606Z"}},"outputs":[{"name":"stdout","text":"Copying gs://adoocr-data/train/X_trainp_pg_vg.npy...\nCopying gs://adoocr-data/train/mapping_labls...                                 \nCopying gs://adoocr-data/train/service-account-key.json...                      \nCopying gs://adoocr-data/train/y_trainp_pg_vg.npy...                            \nCopying gs://adoocr-data/test/X_test_vg.npy...77.3 MiB/s ETA 00:00:00           \nCopying gs://adoocr-data/test/mapping_labls...                                  \nCopying gs://adoocr-data/test/x_test_pg.npy...                                  \nCopying gs://adoocr-data/test/x_testp.npy...                                    \nCopying gs://adoocr-data/test/y_test_pg.npy...                                  \nCopying gs://adoocr-data/test/y_testp.npy...                                    \nCopying gs://adoocr-data/test/y_test_vg.npy...                                  \n- [7/8 files][877.9 MiB/877.9 MiB]  99% Done  65.1 MiB/s ETA 00:00:00           \r","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# %%bash\n# cat <<EOF > /kaggle/working/kaggle.json\n# {\"username\":\"abiywondimu\",\"key\":\"31fd71809a2ceeb88a6e435c42bfed37\"}\n# EOF\n\n# # secure the file\n# chmod 600 /kaggle/working/kaggle.json\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%bash\n# mkdir -p ~/.kaggle\n# mv /kaggle/working/kaggle.json ~/.kaggle/\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !cd /kaggle/working/mydata && kaggle datasets init","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !cd /kaggle/working/mydata\n# import json\n\n# meta_path = \"/kaggle/working/mydata/dataset-metadata.json\"\n# with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n#     data = json.load(f)\n\n# # >>> edit your fields:\n# data[\"title\"] = \"my amharic ocr lines\"\n# data[\"id\"]    = \"abiywondimu/amharic-textline-ocr\"\n# data[\"isPrivate\"] = True\n\n# with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n#     json.dump(data, f, ensure_ascii=False, indent=2)\n# print(\"Re‐wrote metadata!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !cd /kaggle/working/mydata\n\n# !kaggle datasets create -p /kaggle/working/mydata --dir-mode tar","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !kaggle config view","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    VisionEncoderDecoderModel,\n    TrOCRProcessor,\n    TrainingArguments,\n    Trainer,\n    default_data_collator\n)\nimport editdistance","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T15:12:22.227594Z","iopub.execute_input":"2025-06-26T15:12:22.227911Z","iopub.status.idle":"2025-06-26T15:12:53.832553Z","shell.execute_reply.started":"2025-06-26T15:12:22.227886Z","shell.execute_reply":"2025-06-26T15:12:53.831746Z"}},"outputs":[{"name":"stderr","text":"2025-06-26 15:12:38.018183: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750950758.264350      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750950758.340018      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"MODEL_NAME = \"microsoft/trocr-small-printed\"\nBATCH_SIZE = 64            \nEPOCHS     = 2\nLR         = 5e-5\nOUTPUT_DIR = \"./trocr_frozen_head\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T15:13:46.490452Z","iopub.execute_input":"2025-06-26T15:13:46.490728Z","iopub.status.idle":"2025-06-26T15:13:46.494568Z","shell.execute_reply.started":"2025-06-26T15:13:46.490707Z","shell.execute_reply":"2025-06-26T15:13:46.493773Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# ─── 2) DATASET ───────────────────────────────────────────────────────────────\nclass LineOCRDataset(Dataset):\n    def __init__(self, images_np, labels_np, processor, idx2char, max_length=64):\n        self.X = images_np        # shape (N,48,128), dtype uint8 or float32\n        self.Y = labels_np        # shape (N, max_label_len), padded with 0\n        self.proc = processor\n        self.idx2char = idx2char\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, i):\n        img = self.X[i].astype(np.uint8)\n        # TrOCRProcessor expects PIL or np.array H×W×3, so stack channels:\n        img3 = np.stack([img, img, img], axis=-1)\n        pixel_values = self.proc(images=img3, return_tensors=\"pt\").pixel_values[0]\n\n        # decode Y[i] to string\n        seq = self.Y[i]\n        chars = []\n        for idx in seq:\n            if idx == 0: break  # pad\n            chars.append(self.idx2char[int(idx)])\n        text = \"\".join(chars)\n\n        labels = self.proc.tokenizer(\n            text, padding=\"max_length\", truncation=True,\n            max_length=self.max_length, return_tensors=\"pt\"\n        ).input_ids[0]\n\n        return {\n            \"pixel_values\": pixel_values,\n            \"labels\": labels,\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T15:13:54.052450Z","iopub.execute_input":"2025-06-26T15:13:54.052725Z","iopub.status.idle":"2025-06-26T15:13:54.059368Z","shell.execute_reply.started":"2025-06-26T15:13:54.052705Z","shell.execute_reply":"2025-06-26T15:13:54.058204Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# ─── 3) METRICS ───────────────────────────────────────────────────────────────\ndef compute_metrics(pred):\n    pred_ids = pred.predictions.argmax(-1) if pred.predictions.ndim==3 else pred.predictions\n    pred_texts = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    label_ids  = pred.label_ids\n    label_ids[label_ids==-100] = processor.tokenizer.pad_token_id\n    gt_texts   = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n\n    total_cer = total_wer = total_chars = total_words = correct_chars = 0\n    for p, g in zip(pred_texts, gt_texts):\n        # CER\n        total_cer  += editdistance.eval(p, g)\n        total_chars+= len(g)\n        # WER\n        pw = p.split(); gw = g.split()\n        total_wer  += editdistance.eval(pw, gw)\n        total_words+= len(gw)\n        # Char acc\n        for pc, gc in zip(p, g):\n            if pc==gc: correct_chars+=1\n\n    return {\n        \"cer\": total_cer/total_chars,\n        \"wer\": total_wer/total_words,\n        \"char_acc\": correct_chars/total_chars,\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T15:13:58.443545Z","iopub.execute_input":"2025-06-26T15:13:58.443841Z","iopub.status.idle":"2025-06-26T15:13:58.449705Z","shell.execute_reply.started":"2025-06-26T15:13:58.443818Z","shell.execute_reply":"2025-06-26T15:13:58.448797Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# ─── 4) LOAD DATA ─────────────────────────────────────────────────────────────\n# your .npy paths\nX_train = np.load(\"/kaggle/working/mydata/train/X_trainp_pg_vg.npy\", mmap_mode=\"r\")[:200000]\nY_train = np.load(\"/kaggle/working/mydata/train/y_trainp_pg_vg.npy\", allow_pickle=True)[:200000]\nX_val   = np.load(\"/kaggle/working/mydata/test/x_testp.npy\",  mmap_mode=\"r\")\nY_val   = np.load(\"/kaggle/working/mydata/test/y_testp.npy\",   allow_pickle=True)\n\n# build idx2char from your mapping file\nidx2char = {}\n\nspecial_tokens = {\n    \"<space>\": \" \",\n    \"<newline>\": \"\\n\",\n    \"<tab>\": \"\\t\"\n}\n\nwith open(\"/kaggle/working/mydata/train/mapping_labls\", encoding=\"utf-8\") as f:\n    for line in f:\n        parts = line.strip().split(maxsplit=1)\n        if len(parts) == 2:\n            i, ch = parts\n            ch = special_tokens.get(ch, ch)  # convert <space> etc to actual char\n            idx2char[int(i)] = ch\n\n\n\nprocessor = TrOCRProcessor.from_pretrained(MODEL_NAME)\n\ntrain_ds = LineOCRDataset(X_train, Y_train, processor, idx2char)\nval_ds   = LineOCRDataset(X_val,   Y_val,   processor, idx2char)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T15:14:07.343842Z","iopub.execute_input":"2025-06-26T15:14:07.344138Z","iopub.status.idle":"2025-06-26T15:14:10.838828Z","shell.execute_reply.started":"2025-06-26T15:14:07.344115Z","shell.execute_reply":"2025-06-26T15:14:10.837988Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/272 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dffc46f92e143f6aa2dc06dad0de909"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/327 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b795fd6600b64f80bdfbeb0e63829b72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7a50d79fc084990bf82985563a62f67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/238 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3842016113a49c8aaaa75efaac1e004"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# print(\"Sample label indices:\", dataset[0][\"label_ids\"])\n\nprint(\"Available label indices:\", sorted(idx2char.keys()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T15:14:18.510502Z","iopub.execute_input":"2025-06-26T15:14:18.511212Z","iopub.status.idle":"2025-06-26T15:14:18.515358Z","shell.execute_reply.started":"2025-06-26T15:14:18.511187Z","shell.execute_reply":"2025-06-26T15:14:18.514497Z"}},"outputs":[{"name":"stdout","text":"Available label indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279]\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# ─── 5) MODEL ────────────────────────────────────────────────────────────────\nmodel = VisionEncoderDecoderModel.from_pretrained(MODEL_NAME)\n\n# freeze entire encoder\nfor param in model.encoder.parameters():\n    param.requires_grad = False\n\n# set pad token\nmodel.config.decoder_start_token_id = processor.tokenizer.cls_token_id\nmodel.config.pad_token_id           = processor.tokenizer.pad_token_id\nmodel.config.vocab_size             = model.config.decoder.vocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T15:14:22.637036Z","iopub.execute_input":"2025-06-26T15:14:22.637815Z","iopub.status.idle":"2025-06-26T15:14:29.457466Z","shell.execute_reply.started":"2025-06-26T15:14:22.637782Z","shell.execute_reply":"2025-06-26T15:14:29.456632Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a723a34737646f5aaabe27f43116b39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/246M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d874b4753514d10b8f895750aa1be4d"}},"metadata":{}},{"name":"stderr","text":"Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"568558e984d144188911b66156d8c06f"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"from transformers import EarlyStoppingCallback\n\n# ─── 6) TRAINER ──────────────────────────────────────────────────────────────\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    per_device_train_batch_size=BATCH_SIZE,\n    save_total_limit=2,\n    fp16=True,\n    learning_rate=LR,\n    logging_steps=200,\n    report_to=\"none\",\n    save_strategy=\"no\",  # no save if you're not evaluating\n    eval_strategy=\"no\"  # disable eval\n)\n\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_ds,\n    # eval_dataset=val_ds,  # <-- comment or remove this line\n    data_collator=default_data_collator,\n    compute_metrics=None,\n    callbacks=[]\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T15:14:55.808555Z","iopub.execute_input":"2025-06-26T15:14:55.808829Z","iopub.status.idle":"2025-06-26T15:14:55.851935Z","shell.execute_reply.started":"2025-06-26T15:14:55.808810Z","shell.execute_reply":"2025-06-26T15:14:55.851175Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# ─── 7) TRAIN ─────────────────────────────────────────────────────────────────\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T15:15:00.951794Z","iopub.execute_input":"2025-06-26T15:15:00.952303Z","iopub.status.idle":"2025-06-26T18:04:17.420537Z","shell.execute_reply.started":"2025-06-26T15:15:00.952277Z","shell.execute_reply":"2025-06-26T18:04:17.419708Z"}},"outputs":[{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9375' max='9375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9375/9375 2:49:12, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>1.499100</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.736500</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.719800</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.708700</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.700400</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.689700</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.683800</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.677800</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>0.671400</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.663100</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>0.659900</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>0.655000</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>0.649600</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.643000</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.637900</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.632200</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.626100</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.620700</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.616200</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.611800</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>0.604000</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>0.597500</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>0.597100</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>0.591600</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.584700</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>0.583000</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>0.575300</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>0.569900</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>0.569600</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.566000</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>0.561700</td>\n    </tr>\n    <tr>\n      <td>6400</td>\n      <td>0.556000</td>\n    </tr>\n    <tr>\n      <td>6600</td>\n      <td>0.551300</td>\n    </tr>\n    <tr>\n      <td>6800</td>\n      <td>0.552900</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.549400</td>\n    </tr>\n    <tr>\n      <td>7200</td>\n      <td>0.546600</td>\n    </tr>\n    <tr>\n      <td>7400</td>\n      <td>0.543400</td>\n    </tr>\n    <tr>\n      <td>7600</td>\n      <td>0.542200</td>\n    </tr>\n    <tr>\n      <td>7800</td>\n      <td>0.538300</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.537800</td>\n    </tr>\n    <tr>\n      <td>8200</td>\n      <td>0.537200</td>\n    </tr>\n    <tr>\n      <td>8400</td>\n      <td>0.536100</td>\n    </tr>\n    <tr>\n      <td>8600</td>\n      <td>0.531500</td>\n    </tr>\n    <tr>\n      <td>8800</td>\n      <td>0.532500</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.531700</td>\n    </tr>\n    <tr>\n      <td>9200</td>\n      <td>0.528100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=9375, training_loss=0.6204124739583333, metrics={'train_runtime': 10155.9596, 'train_samples_per_second': 59.079, 'train_steps_per_second': 0.923, 'total_flos': 7.17748124516352e+19, 'train_loss': 0.6204124739583333, 'epoch': 3.0})"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"# ─── 8) SAVE ──────────────────────────────────────────────────────────────────\ntrainer.save_model(OUTPUT_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T18:06:24.783770Z","iopub.execute_input":"2025-06-26T18:06:24.784562Z","iopub.status.idle":"2025-06-26T18:06:25.617489Z","shell.execute_reply.started":"2025-06-26T18:06:24.784533Z","shell.execute_reply":"2025-06-26T18:06:25.616550Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"\n\n# 9) LOAD back for eval\nfrom transformers import VisionEncoderDecoderModel, TrOCRProcessor\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = VisionEncoderDecoderModel.from_pretrained(OUTPUT_DIR)\nmodel.to(device).eval()\n\nprocessor = TrOCRProcessor.from_pretrained(OUTPUT_DIR)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 10) STREAMING VALIDATION (as before)\nfrom torch.utils.data import DataLoader\nimport time\n\neval_loader = DataLoader(val_ds, batch_size=32, shuffle=False, pin_memory=True, num_workers=2)\n\nall_preds, all_refs = [], []\nt0 = time.time()\n\nfor step, batch in enumerate(eval_loader, 1):\n    pixels = batch[\"pixel_values\"].to(device)\n    labels = batch[\"labels\"].numpy()\n\n    with torch.no_grad():\n        gen_ids = model.generate(pixels)\n    preds = processor.batch_decode(gen_ids, skip_special_tokens=True)\n\n    refs = []\n    for lab in labels:\n        lab = [i for i in lab if i != processor.tokenizer.pad_token_id]\n        refs.append(\"\".join(idx2char[i] for i in lab))\n\n    all_preds.extend(preds)\n    all_refs .extend(refs)\n\n    if step % 100 == 0 or step == len(eval_loader):\n        cer  = compute_cer(all_preds, all_refs)\n        wer  = compute_wer(all_preds, all_refs)\n        acc  = compute_char_accuracy(all_preds, all_refs)\n        elapsed = (time.time() - t0) / 60\n        pct = 100 * step / len(eval_loader)\n        print(f\"[{step}/{len(eval_loader)} {pct:.1f}%] \"\n              f\"{elapsed:.1f}m elapsed  CER:{cer:.3f}  WER:{wer:.3f}  ACC:{acc:.3f}\")\n\ntotal_time = (time.time() - t0) / 60\ncer  = compute_cer(all_preds, all_refs)\nwer  = compute_wer(all_preds, all_refs)\nacc  = compute_char_accuracy(all_preds, all_refs)\nprint(f\"\\nDONE in {total_time:.1f}m  →  CER:{cer:.4f}  WER:{wer:.4f}  CharAcc:{acc:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r /kaggle/working/trocr_frozen_head.zip /kaggle/working/trocr_frozen_head","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T18:06:49.161404Z","iopub.execute_input":"2025-06-26T18:06:49.161679Z","iopub.status.idle":"2025-06-26T18:07:30.389160Z","shell.execute_reply.started":"2025-06-26T18:06:49.161657Z","shell.execute_reply":"2025-06-26T18:07:30.388470Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/trocr_frozen_head/ (stored 0%)\n  adding: kaggle/working/trocr_frozen_head/training_args.bin (deflated 52%)\n  adding: kaggle/working/trocr_frozen_head/model.safetensors","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":" (deflated 28%)\n  adding: kaggle/working/trocr_frozen_head/config.json (deflated 65%)\n  adding: kaggle/working/trocr_frozen_head/generation_config.json (deflated 35%)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"!pip install -q google-cloud-storage","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T18:08:10.531340Z","iopub.execute_input":"2025-06-26T18:08:10.531649Z","iopub.status.idle":"2025-06-26T18:08:27.372735Z","shell.execute_reply.started":"2025-06-26T18:08:10.531624Z","shell.execute_reply":"2025-06-26T18:08:27.371984Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.8/160.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.25.1 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"!gsutil -m cp -r gs://adoocr-data/train/service-account-key.json /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T18:09:27.588519Z","iopub.execute_input":"2025-06-26T18:09:27.588794Z","iopub.status.idle":"2025-06-26T18:09:29.092488Z","shell.execute_reply.started":"2025-06-26T18:09:27.588772Z","shell.execute_reply":"2025-06-26T18:09:29.091531Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Copying gs://adoocr-data/train/service-account-key.json...\n/ [1/1 files][  2.3 KiB/  2.3 KiB] 100% Done                                    \nOperation completed over 1 objects/2.3 KiB.                                      \n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"from google.cloud import storage\nimport os\n\n# Path to service account key\nkey_path = \"/kaggle/working/service-account-key.json\"\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = key_path\n\n# Initialize GCS client\nclient = storage.Client()\nbucket_name = 'adoocr-data'  # TODO: Replace this with your actual bucket name\ndestination_blob_name = '/ll/trocr_frozen_head.zip'\nsource_file_name = '/kaggle/working/trocr_frozen_head.zip'\n\n# Upload file\nbucket = client.get_bucket(bucket_name)\nblob = bucket.blob(destination_blob_name)\nblob.upload_from_filename(source_file_name)\n\nprint(\"✅ Upload to GCP completed!\")\nprint(f\"🔗 File URL: https://storage.googleapis.com/{bucket_name}/{destination_blob_name}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T18:14:39.358732Z","iopub.execute_input":"2025-06-26T18:14:39.358940Z","iopub.status.idle":"2025-06-26T18:14:46.368389Z","shell.execute_reply.started":"2025-06-26T18:14:39.358924Z","shell.execute_reply":"2025-06-26T18:14:46.367601Z"}},"outputs":[{"name":"stdout","text":"✅ Upload to GCP completed!\n🔗 File URL: https://storage.googleapis.com/adoocr-data//ll/trocr_frozen_head.zip\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"for blob in bucket.list_blobs():\n    print(blob.name)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}