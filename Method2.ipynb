{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejjcoDj4E5nH",
        "outputId": "106de5ae-9234-4d8e-8a18-0cb49afc1503"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "libboost-all-dev is already the newest version (1.74.0.3ubuntu7).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
            "  Downloading https://github.com/kpu/kenlm/archive/master.zip\n",
            "\u001b[2K     \u001b[32m\\\u001b[0m \u001b[32m553.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: kenlm\n",
            "  Building wheel for kenlm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kenlm: filename=kenlm-0.2.0-cp311-cp311-linux_x86_64.whl size=3185076 sha256=db3218a88d77d4022ad31aa69821468908cc908e8e7e21136d884fcd8ca98df5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-s7jjbvsx/wheels/4e/ca/6a/e5da175b1396483f6f410cdb4cfe8bc8fa5e12088e91d60413\n",
            "Successfully built kenlm\n",
            "Installing collected packages: kenlm\n",
            "Successfully installed kenlm-0.2.0\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y build-essential libboost-all-dev\n",
        "!pip install https://github.com/kpu/kenlm/archive/master.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer editdistance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFt6Ign1Ful3",
        "outputId": "36b366d3-2dad-4410-d02a-e19c9f33e3b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jiwer\n",
            "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.2.1)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-4.0.0 rapidfuzz-3.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your labels.csv file from line-based data\n",
        "csv_path = '/content/labels.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Filter only line-based training data\n",
        "line_df = df[(df['type'] == 'text-line') & (df['split'] == 'train')]\n",
        "\n",
        "# Extract labels (text lines)\n",
        "lines = line_df['label'].dropna().astype(str).tolist()\n",
        "\n",
        "# Save to train_text.txt\n",
        "with open('train_text.txt', 'w', encoding='utf-8') as f:\n",
        "    for line in lines:\n",
        "        f.write(line.strip() + '\\n')\n",
        "\n",
        "print(f\" Saved {len(lines)} lines to train_text.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQboVKcsG_Bb",
        "outputId": "a32d5aff-31b9-4125-b75e-f669c03b9ac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved 32212 lines to train_text.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y build-essential libboost-all-dev cmake unzip\n",
        "!wget https://github.com/kpu/kenlm/archive/master.zip -O kenlm.zip\n",
        "!unzip kenlm.zip\n",
        "%cd kenlm-master\n",
        "!mkdir -p build && cd build && cmake .. && make -j4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-pJ2UN0H-GB",
        "outputId": "552bf28b-ecba-48b9-8276-f61d2d648bdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connected to cloud.r-project.org (18.160.213.79)] [Co\r                                                                               \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,561 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,050 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,726 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,347 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,747 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,253 kB]\n",
            "Fetched 23.1 MB in 3s (7,800 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "libboost-all-dev is already the newest version (1.74.0.3ubuntu7).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "unzip is already the newest version (6.0-26ubuntu3.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 36 not upgraded.\n",
            "--2025-06-24 09:12:14--  https://github.com/kpu/kenlm/archive/master.zip\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/kpu/kenlm/zip/refs/heads/master [following]\n",
            "--2025-06-24 09:12:14--  https://codeload.github.com/kpu/kenlm/zip/refs/heads/master\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.113.10\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.113.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘kenlm.zip’\n",
            "\n",
            "kenlm.zip               [ <=>                ] 540.63K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-06-24 09:12:15 (3.53 MB/s) - ‘kenlm.zip’ saved [553608]\n",
            "\n",
            "Archive:  kenlm.zip\n",
            "4cb443e60b7bf2c0ddf3c745378f76cb59e254e5\n",
            "   creating: kenlm-master/\n",
            "   creating: kenlm-master/.github/\n",
            "   creating: kenlm-master/.github/workflows/\n",
            "  inflating: kenlm-master/.github/workflows/mac.yml  \n",
            "  inflating: kenlm-master/.github/workflows/ubuntu.yml  \n",
            "  inflating: kenlm-master/.github/workflows/windows.yml  \n",
            "  inflating: kenlm-master/.gitignore  \n",
            "  inflating: kenlm-master/BUILDING   \n",
            "  inflating: kenlm-master/CMakeLists.txt  \n",
            "  inflating: kenlm-master/COPYING    \n",
            "  inflating: kenlm-master/COPYING.3  \n",
            "  inflating: kenlm-master/COPYING.LESSER.3  \n",
            "  inflating: kenlm-master/Doxyfile   \n",
            "  inflating: kenlm-master/LICENSE    \n",
            "  inflating: kenlm-master/MANIFEST.in  \n",
            "  inflating: kenlm-master/README.md  \n",
            "  inflating: kenlm-master/clean_query_only.sh  \n",
            "   creating: kenlm-master/cmake/\n",
            "  inflating: kenlm-master/cmake/KenLMFunctions.cmake  \n",
            "  inflating: kenlm-master/cmake/kenlmConfig.cmake.in  \n",
            "   creating: kenlm-master/cmake/modules/\n",
            "  inflating: kenlm-master/cmake/modules/FindEigen3.cmake  \n",
            "  inflating: kenlm-master/compile_query_only.sh  \n",
            "   creating: kenlm-master/lm/\n",
            "  inflating: kenlm-master/lm/CMakeLists.txt  \n",
            "  inflating: kenlm-master/lm/bhiksha.cc  \n",
            "  inflating: kenlm-master/lm/bhiksha.hh  \n",
            "  inflating: kenlm-master/lm/binary_format.cc  \n",
            "  inflating: kenlm-master/lm/binary_format.hh  \n",
            "  inflating: kenlm-master/lm/blank.hh  \n",
            "  inflating: kenlm-master/lm/build_binary_main.cc  \n",
            "   creating: kenlm-master/lm/builder/\n",
            "  inflating: kenlm-master/lm/builder/CMakeLists.txt  \n",
            "  inflating: kenlm-master/lm/builder/README.md  \n",
            "  inflating: kenlm-master/lm/builder/TODO  \n",
            "  inflating: kenlm-master/lm/builder/adjust_counts.cc  \n",
            "  inflating: kenlm-master/lm/builder/adjust_counts.hh  \n",
            "  inflating: kenlm-master/lm/builder/adjust_counts_test.cc  \n",
            "  inflating: kenlm-master/lm/builder/combine_counts.hh  \n",
            "  inflating: kenlm-master/lm/builder/corpus_count.cc  \n",
            "  inflating: kenlm-master/lm/builder/corpus_count.hh  \n",
            "  inflating: kenlm-master/lm/builder/corpus_count_test.cc  \n",
            "  inflating: kenlm-master/lm/builder/count_ngrams_main.cc  \n",
            "  inflating: kenlm-master/lm/builder/debug_print.hh  \n",
            "  inflating: kenlm-master/lm/builder/discount.hh  \n",
            "  inflating: kenlm-master/lm/builder/dump_counts_main.cc  \n",
            "  inflating: kenlm-master/lm/builder/hash_gamma.hh  \n",
            "  inflating: kenlm-master/lm/builder/header_info.hh  \n",
            "  inflating: kenlm-master/lm/builder/initial_probabilities.cc  \n",
            "  inflating: kenlm-master/lm/builder/initial_probabilities.hh  \n",
            "  inflating: kenlm-master/lm/builder/interpolate.cc  \n",
            "  inflating: kenlm-master/lm/builder/interpolate.hh  \n",
            "  inflating: kenlm-master/lm/builder/lmplz_main.cc  \n",
            "  inflating: kenlm-master/lm/builder/output.cc  \n",
            "  inflating: kenlm-master/lm/builder/output.hh  \n",
            "  inflating: kenlm-master/lm/builder/payload.hh  \n",
            "  inflating: kenlm-master/lm/builder/pipeline.cc  \n",
            "  inflating: kenlm-master/lm/builder/pipeline.hh  \n",
            "   creating: kenlm-master/lm/common/\n",
            "  inflating: kenlm-master/lm/common/CMakeLists.txt  \n",
            "  inflating: kenlm-master/lm/common/compare.hh  \n",
            "  inflating: kenlm-master/lm/common/joint_order.hh  \n",
            "  inflating: kenlm-master/lm/common/model_buffer.cc  \n",
            "  inflating: kenlm-master/lm/common/model_buffer.hh  \n",
            "  inflating: kenlm-master/lm/common/model_buffer_test.cc  \n",
            "  inflating: kenlm-master/lm/common/ngram.hh  \n",
            "  inflating: kenlm-master/lm/common/ngram_stream.hh  \n",
            "  inflating: kenlm-master/lm/common/print.cc  \n",
            "  inflating: kenlm-master/lm/common/print.hh  \n",
            "  inflating: kenlm-master/lm/common/renumber.cc  \n",
            "  inflating: kenlm-master/lm/common/renumber.hh  \n",
            "  inflating: kenlm-master/lm/common/size_option.cc  \n",
            "  inflating: kenlm-master/lm/common/size_option.hh  \n",
            "  inflating: kenlm-master/lm/common/special.hh  \n",
            "   creating: kenlm-master/lm/common/test_data/\n",
            "   creating: kenlm-master/lm/common/test_data/bigendian/\n",
            "  inflating: kenlm-master/lm/common/test_data/bigendian/toy0.1  \n",
            "  inflating: kenlm-master/lm/common/test_data/bigendian/toy0.2  \n",
            "  inflating: kenlm-master/lm/common/test_data/bigendian/toy0.3  \n",
            " extracting: kenlm-master/lm/common/test_data/bigendian/toy0.kenlm_intermediate  \n",
            " extracting: kenlm-master/lm/common/test_data/bigendian/toy0.vocab  \n",
            "  inflating: kenlm-master/lm/common/test_data/bigendian/toy1.1  \n",
            "  inflating: kenlm-master/lm/common/test_data/bigendian/toy1.2  \n",
            "  inflating: kenlm-master/lm/common/test_data/bigendian/toy1.3  \n",
            " extracting: kenlm-master/lm/common/test_data/bigendian/toy1.kenlm_intermediate  \n",
            " extracting: kenlm-master/lm/common/test_data/bigendian/toy1.vocab  \n",
            "  inflating: kenlm-master/lm/common/test_data/generate.sh  \n",
            "   creating: kenlm-master/lm/common/test_data/littleendian/\n",
            "  inflating: kenlm-master/lm/common/test_data/littleendian/toy0.1  \n",
            "  inflating: kenlm-master/lm/common/test_data/littleendian/toy0.2  \n",
            "  inflating: kenlm-master/lm/common/test_data/littleendian/toy0.3  \n",
            " extracting: kenlm-master/lm/common/test_data/littleendian/toy0.kenlm_intermediate  \n",
            " extracting: kenlm-master/lm/common/test_data/littleendian/toy0.vocab  \n",
            "  inflating: kenlm-master/lm/common/test_data/littleendian/toy1.1  \n",
            "  inflating: kenlm-master/lm/common/test_data/littleendian/toy1.2  \n",
            "  inflating: kenlm-master/lm/common/test_data/littleendian/toy1.3  \n",
            " extracting: kenlm-master/lm/common/test_data/littleendian/toy1.kenlm_intermediate  \n",
            " extracting: kenlm-master/lm/common/test_data/littleendian/toy1.vocab  \n",
            "  inflating: kenlm-master/lm/common/test_data/toy0.arpa  \n",
            "  inflating: kenlm-master/lm/common/test_data/toy1.arpa  \n",
            "  inflating: kenlm-master/lm/config.cc  \n",
            "  inflating: kenlm-master/lm/config.hh  \n",
            "  inflating: kenlm-master/lm/enumerate_vocab.hh  \n",
            "  inflating: kenlm-master/lm/facade.hh  \n",
            "   creating: kenlm-master/lm/filter/\n",
            "  inflating: kenlm-master/lm/filter/CMakeLists.txt  \n",
            "  inflating: kenlm-master/lm/filter/arpa_io.cc  \n",
            "  inflating: kenlm-master/lm/filter/arpa_io.hh  \n",
            "  inflating: kenlm-master/lm/filter/count_io.hh  \n",
            "  inflating: kenlm-master/lm/filter/filter_main.cc  \n",
            "  inflating: kenlm-master/lm/filter/format.hh  \n",
            "  inflating: kenlm-master/lm/filter/phrase.cc  \n",
            "  inflating: kenlm-master/lm/filter/phrase.hh  \n",
            "  inflating: kenlm-master/lm/filter/phrase_table_vocab_main.cc  \n",
            "  inflating: kenlm-master/lm/filter/thread.hh  \n",
            "  inflating: kenlm-master/lm/filter/vocab.cc  \n",
            "  inflating: kenlm-master/lm/filter/vocab.hh  \n",
            "  inflating: kenlm-master/lm/filter/wrapper.hh  \n",
            "  inflating: kenlm-master/lm/fragment_main.cc  \n",
            "   creating: kenlm-master/lm/interpolate/\n",
            "  inflating: kenlm-master/lm/interpolate/CMakeLists.txt  \n",
            "  inflating: kenlm-master/lm/interpolate/backoff_matrix.hh  \n",
            "  inflating: kenlm-master/lm/interpolate/backoff_reunification.cc  \n",
            "  inflating: kenlm-master/lm/interpolate/backoff_reunification.hh  \n",
            "  inflating: kenlm-master/lm/interpolate/backoff_reunification_test.cc  \n",
            "  inflating: kenlm-master/lm/interpolate/bounded_sequence_encoding.cc  \n",
            "  inflating: kenlm-master/lm/interpolate/bounded_sequence_encoding.hh  \n",
            "  inflating: kenlm-master/lm/interpolate/bounded_sequence_encoding_test.cc  \n",
            "  inflating: kenlm-master/lm/interpolate/interpolate_info.hh  \n",
            "  inflating: kenlm-master/lm/interpolate/interpolate_main.cc  \n",
            "  inflating: kenlm-master/lm/interpolate/merge_probabilities.cc  \n",
            "  inflating: kenlm-master/lm/interpolate/merge_probabilities.hh  \n",
            "  inflating: kenlm-master/lm/interpolate/merge_vocab.cc  \n",
            "  inflating: kenlm-master/lm/interpolate/merge_vocab.hh  \n",
            "  inflating: kenlm-master/lm/interpolate/merge_vocab_test.cc  \n",
            "  inflating: kenlm-master/lm/interpolate/normalize.cc  \n",
            "  inflating: kenlm-master/lm/interpolate/normalize.hh  \n",
            "  inflating: kenlm-master/lm/interpolate/normalize_test.cc  \n",
            "  inflating: kenlm-master/lm/interpolate/pipeline.cc  \n",
            "  inflating: kenlm-master/lm/interpolate/pipeline.hh  \n",
            "  inflating: kenlm-master/lm/interpolate/split_worker.cc  \n",
            "  inflating: kenlm-master/lm/interpolate/split_worker.hh  \n",
            "  inflating: kenlm-master/lm/interpolate/streaming_example_main.cc  \n",
            "  inflating: kenlm-master/lm/interpolate/tune_derivatives.cc  \n",
            "  inflating: kenlm-master/lm/interpolate/tune_derivatives.hh  \n",
            "  inflating: kenlm-master/lm/interpolate/tune_derivatives_test.cc  \n",
            "  inflating: kenlm-master/lm/interpolate/tune_instances.cc  \n",
            "  inflating: kenlm-master/lm/interpolate/tune_instances.hh  \n",
            "  inflating: kenlm-master/lm/interpolate/tune_instances_test.cc  \n",
            "  inflating: kenlm-master/lm/interpolate/tune_matrix.hh  \n",
            "  inflating: kenlm-master/lm/interpolate/tune_weights.cc  \n",
            "  inflating: kenlm-master/lm/interpolate/tune_weights.hh  \n",
            "  inflating: kenlm-master/lm/interpolate/universal_vocab.cc  \n",
            "  inflating: kenlm-master/lm/interpolate/universal_vocab.hh  \n",
            "  inflating: kenlm-master/lm/kenlm_benchmark_main.cc  \n",
            "  inflating: kenlm-master/lm/left.hh  \n",
            "  inflating: kenlm-master/lm/left_test.cc  \n",
            "  inflating: kenlm-master/lm/lm_exception.cc  \n",
            "  inflating: kenlm-master/lm/lm_exception.hh  \n",
            "  inflating: kenlm-master/lm/max_order.hh  \n",
            "  inflating: kenlm-master/lm/model.cc  \n",
            "  inflating: kenlm-master/lm/model.hh  \n",
            "  inflating: kenlm-master/lm/model_test.cc  \n",
            "  inflating: kenlm-master/lm/model_type.hh  \n",
            "  inflating: kenlm-master/lm/ngram_query.hh  \n",
            "  inflating: kenlm-master/lm/partial.hh  \n",
            "  inflating: kenlm-master/lm/partial_test.cc  \n",
            "  inflating: kenlm-master/lm/quantize.cc  \n",
            "  inflating: kenlm-master/lm/quantize.hh  \n",
            "  inflating: kenlm-master/lm/query_main.cc  \n",
            "  inflating: kenlm-master/lm/read_arpa.cc  \n",
            "  inflating: kenlm-master/lm/read_arpa.hh  \n",
            "  inflating: kenlm-master/lm/return.hh  \n",
            "  inflating: kenlm-master/lm/search_hashed.cc  \n",
            "  inflating: kenlm-master/lm/search_hashed.hh  \n",
            "  inflating: kenlm-master/lm/search_trie.cc  \n",
            "  inflating: kenlm-master/lm/search_trie.hh  \n",
            "  inflating: kenlm-master/lm/sizes.cc  \n",
            "  inflating: kenlm-master/lm/sizes.hh  \n",
            "  inflating: kenlm-master/lm/state.hh  \n",
            "  inflating: kenlm-master/lm/test.arpa  \n",
            "  inflating: kenlm-master/lm/test_nounk.arpa  \n",
            "  inflating: kenlm-master/lm/trie.cc  \n",
            "  inflating: kenlm-master/lm/trie.hh  \n",
            "  inflating: kenlm-master/lm/trie_sort.cc  \n",
            "  inflating: kenlm-master/lm/trie_sort.hh  \n",
            "  inflating: kenlm-master/lm/value.hh  \n",
            "  inflating: kenlm-master/lm/value_build.cc  \n",
            "  inflating: kenlm-master/lm/value_build.hh  \n",
            "  inflating: kenlm-master/lm/virtual_interface.cc  \n",
            "  inflating: kenlm-master/lm/virtual_interface.hh  \n",
            "  inflating: kenlm-master/lm/vocab.cc  \n",
            "  inflating: kenlm-master/lm/vocab.hh  \n",
            "  inflating: kenlm-master/lm/weights.hh  \n",
            "  inflating: kenlm-master/lm/word_index.hh  \n",
            "   creating: kenlm-master/lm/wrappers/\n",
            "  inflating: kenlm-master/lm/wrappers/README  \n",
            "  inflating: kenlm-master/lm/wrappers/nplm.cc  \n",
            "  inflating: kenlm-master/lm/wrappers/nplm.hh  \n",
            "  inflating: kenlm-master/pyproject.toml  \n",
            "   creating: kenlm-master/python/\n",
            "  inflating: kenlm-master/python/BuildStandalone.cmake  \n",
            "  inflating: kenlm-master/python/CMakeLists.txt  \n",
            "  inflating: kenlm-master/python/_kenlm.pxd  \n",
            "  inflating: kenlm-master/python/example.py  \n",
            "  inflating: kenlm-master/python/kenlm.cpp  \n",
            "  inflating: kenlm-master/python/kenlm.pyx  \n",
            "  inflating: kenlm-master/python/score_sentence.cc  \n",
            "  inflating: kenlm-master/python/score_sentence.hh  \n",
            "  inflating: kenlm-master/setup.py   \n",
            "   creating: kenlm-master/util/\n",
            "  inflating: kenlm-master/util/CMakeLists.txt  \n",
            "  inflating: kenlm-master/util/bit_packing.cc  \n",
            "  inflating: kenlm-master/util/bit_packing.hh  \n",
            "  inflating: kenlm-master/util/bit_packing_test.cc  \n",
            "  inflating: kenlm-master/util/cat_compressed_main.cc  \n",
            "   creating: kenlm-master/util/double-conversion/\n",
            "  inflating: kenlm-master/util/double-conversion/CMakeLists.txt  \n",
            "  inflating: kenlm-master/util/double-conversion/LICENSE  \n",
            "  inflating: kenlm-master/util/double-conversion/bignum-dtoa.cc  \n",
            "  inflating: kenlm-master/util/double-conversion/bignum-dtoa.h  \n",
            "  inflating: kenlm-master/util/double-conversion/bignum.cc  \n",
            "  inflating: kenlm-master/util/double-conversion/bignum.h  \n",
            "  inflating: kenlm-master/util/double-conversion/cached-powers.cc  \n",
            "  inflating: kenlm-master/util/double-conversion/cached-powers.h  \n",
            "  inflating: kenlm-master/util/double-conversion/diy-fp.h  \n",
            "  inflating: kenlm-master/util/double-conversion/double-conversion.h  \n",
            "  inflating: kenlm-master/util/double-conversion/double-to-string.cc  \n",
            "  inflating: kenlm-master/util/double-conversion/double-to-string.h  \n",
            "  inflating: kenlm-master/util/double-conversion/fast-dtoa.cc  \n",
            "  inflating: kenlm-master/util/double-conversion/fast-dtoa.h  \n",
            "  inflating: kenlm-master/util/double-conversion/fixed-dtoa.cc  \n",
            "  inflating: kenlm-master/util/double-conversion/fixed-dtoa.h  \n",
            "  inflating: kenlm-master/util/double-conversion/ieee.h  \n",
            "  inflating: kenlm-master/util/double-conversion/string-to-double.cc  \n",
            "  inflating: kenlm-master/util/double-conversion/string-to-double.h  \n",
            "  inflating: kenlm-master/util/double-conversion/strtod.cc  \n",
            "  inflating: kenlm-master/util/double-conversion/strtod.h  \n",
            "  inflating: kenlm-master/util/double-conversion/utils.h  \n",
            "  inflating: kenlm-master/util/ersatz_progress.cc  \n",
            "  inflating: kenlm-master/util/ersatz_progress.hh  \n",
            "  inflating: kenlm-master/util/exception.cc  \n",
            "  inflating: kenlm-master/util/exception.hh  \n",
            "  inflating: kenlm-master/util/fake_ostream.hh  \n",
            "  inflating: kenlm-master/util/file.cc  \n",
            "  inflating: kenlm-master/util/file.hh  \n",
            "  inflating: kenlm-master/util/file_piece.cc  \n",
            "  inflating: kenlm-master/util/file_piece.hh  \n",
            "  inflating: kenlm-master/util/file_piece_test.cc  \n",
            "  inflating: kenlm-master/util/file_stream.hh  \n",
            "  inflating: kenlm-master/util/fixed_array.hh  \n",
            "  inflating: kenlm-master/util/float_to_string.cc  \n",
            "  inflating: kenlm-master/util/float_to_string.hh  \n",
            "  inflating: kenlm-master/util/getopt.c  \n",
            "  inflating: kenlm-master/util/getopt.hh  \n",
            "  inflating: kenlm-master/util/have.hh  \n",
            "  inflating: kenlm-master/util/integer_to_string.cc  \n",
            "  inflating: kenlm-master/util/integer_to_string.hh  \n",
            "  inflating: kenlm-master/util/integer_to_string_test.cc  \n",
            "  inflating: kenlm-master/util/joint_sort.hh  \n",
            "  inflating: kenlm-master/util/joint_sort_test.cc  \n",
            "  inflating: kenlm-master/util/mmap.cc  \n",
            "  inflating: kenlm-master/util/mmap.hh  \n",
            "  inflating: kenlm-master/util/multi_intersection.hh  \n",
            "  inflating: kenlm-master/util/multi_intersection_test.cc  \n",
            "  inflating: kenlm-master/util/murmur_hash.cc  \n",
            "  inflating: kenlm-master/util/murmur_hash.hh  \n",
            "  inflating: kenlm-master/util/parallel_read.cc  \n",
            "  inflating: kenlm-master/util/parallel_read.hh  \n",
            "  inflating: kenlm-master/util/pcqueue.hh  \n",
            "  inflating: kenlm-master/util/pcqueue_test.cc  \n",
            "  inflating: kenlm-master/util/pool.cc  \n",
            "  inflating: kenlm-master/util/pool.hh  \n",
            "  inflating: kenlm-master/util/probing_hash_table.hh  \n",
            "  inflating: kenlm-master/util/probing_hash_table_benchmark_main.cc  \n",
            "  inflating: kenlm-master/util/probing_hash_table_test.cc  \n",
            "  inflating: kenlm-master/util/proxy_iterator.hh  \n",
            "  inflating: kenlm-master/util/read_compressed.cc  \n",
            "  inflating: kenlm-master/util/read_compressed.hh  \n",
            "  inflating: kenlm-master/util/read_compressed_test.cc  \n",
            "  inflating: kenlm-master/util/scoped.cc  \n",
            "  inflating: kenlm-master/util/scoped.hh  \n",
            "  inflating: kenlm-master/util/sized_iterator.hh  \n",
            "  inflating: kenlm-master/util/sized_iterator_test.cc  \n",
            "  inflating: kenlm-master/util/sorted_uniform.hh  \n",
            "  inflating: kenlm-master/util/sorted_uniform_test.cc  \n",
            "  inflating: kenlm-master/util/spaces.cc  \n",
            "  inflating: kenlm-master/util/spaces.hh  \n",
            "   creating: kenlm-master/util/stream/\n",
            "  inflating: kenlm-master/util/stream/CMakeLists.txt  \n",
            "  inflating: kenlm-master/util/stream/block.hh  \n",
            "  inflating: kenlm-master/util/stream/chain.cc  \n",
            "  inflating: kenlm-master/util/stream/chain.hh  \n",
            "  inflating: kenlm-master/util/stream/config.hh  \n",
            "  inflating: kenlm-master/util/stream/count_records.cc  \n",
            "  inflating: kenlm-master/util/stream/count_records.hh  \n",
            "  inflating: kenlm-master/util/stream/io.cc  \n",
            "  inflating: kenlm-master/util/stream/io.hh  \n",
            "  inflating: kenlm-master/util/stream/io_test.cc  \n",
            "  inflating: kenlm-master/util/stream/line_input.cc  \n",
            "  inflating: kenlm-master/util/stream/line_input.hh  \n",
            "  inflating: kenlm-master/util/stream/multi_progress.cc  \n",
            "  inflating: kenlm-master/util/stream/multi_progress.hh  \n",
            "  inflating: kenlm-master/util/stream/multi_stream.hh  \n",
            "  inflating: kenlm-master/util/stream/rewindable_stream.cc  \n",
            "  inflating: kenlm-master/util/stream/rewindable_stream.hh  \n",
            "  inflating: kenlm-master/util/stream/rewindable_stream_test.cc  \n",
            "  inflating: kenlm-master/util/stream/sort.hh  \n",
            "  inflating: kenlm-master/util/stream/sort_test.cc  \n",
            "  inflating: kenlm-master/util/stream/stream.hh  \n",
            "  inflating: kenlm-master/util/stream/stream_test.cc  \n",
            "  inflating: kenlm-master/util/stream/typed_stream.hh  \n",
            "  inflating: kenlm-master/util/string_piece.cc  \n",
            "  inflating: kenlm-master/util/string_piece.hh  \n",
            "  inflating: kenlm-master/util/string_piece_hash.hh  \n",
            "  inflating: kenlm-master/util/string_stream.hh  \n",
            "  inflating: kenlm-master/util/string_stream_test.cc  \n",
            "  inflating: kenlm-master/util/thread_pool.hh  \n",
            "  inflating: kenlm-master/util/tokenize_piece.hh  \n",
            "  inflating: kenlm-master/util/tokenize_piece_test.cc  \n",
            "  inflating: kenlm-master/util/usage.cc  \n",
            "  inflating: kenlm-master/util/usage.hh  \n",
            "/content/kenlm-master\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Could NOT find Eigen3 (missing: Eigen3_DIR)\n",
            "\u001b[33mCMake Warning (dev) at CMakeLists.txt:101 (find_package):\n",
            "  Policy CMP0167 is not set: The FindBoost module is removed.  Run \"cmake\n",
            "  --help-policy CMP0167\" for policy details.  Use the cmake_policy command to\n",
            "  set the policy and suppress this warning.\n",
            "\n",
            "This warning is for project developers.  Use -Wno-dev to suppress it.\n",
            "\u001b[0m\n",
            "-- Found Boost: /usr/lib/x86_64-linux-gnu/cmake/Boost-1.74.0/BoostConfig.cmake (found suitable version \"1.74.0\", minimum required is \"1.41.0\") found components: program_options system thread unit_test_framework\n",
            "-- Found Threads: TRUE\n",
            "-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\")\n",
            "-- Found BZip2: /usr/lib/x86_64-linux-gnu/libbz2.so (found version \"1.0.8\")\n",
            "-- Looking for BZ2_bzCompressInit\n",
            "-- Looking for BZ2_bzCompressInit - found\n",
            "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Found LibLZMA: /usr/lib/x86_64-linux-gnu/liblzma.so (found version \"5.2.5\")\n",
            "-- Looking for clock_gettime in rt\n",
            "-- Looking for clock_gettime in rt - found\n",
            "-- Configuring done (1.4s)\n",
            "-- Generating done (0.1s)\n",
            "-- Build files have been written to: /content/kenlm-master/build\n",
            "[  1%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum.cc.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum-dtoa.cc.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fast-dtoa.cc.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/cached-powers.cc.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fixed-dtoa.cc.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/strtod.cc.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/double-to-string.cc.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/string-to-double.cc.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/chain.cc.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/count_records.cc.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/io.cc.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/line_input.cc.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/multi_progress.cc.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/rewindable_stream.cc.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/bit_packing.cc.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/ersatz_progress.cc.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/exception.cc.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file.cc.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file_piece.cc.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/float_to_string.cc.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/integer_to_string.cc.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/mmap.cc.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/murmur_hash.cc.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/parallel_read.cc.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/pool.cc.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/read_compressed.cc.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/scoped.cc.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/spaces.cc.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/string_piece.cc.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/usage.cc.o\u001b[0m\n",
            "[ 38%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm_util.a\u001b[0m\n",
            "[ 38%] Built target kenlm_util\n",
            "[ 40%] \u001b[32mBuilding CXX object util/CMakeFiles/probing_hash_table_benchmark.dir/probing_hash_table_benchmark_main.cc.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/binary_format.cc.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/bhiksha.cc.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/arpa_io.cc.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/config.cc.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/lm_exception.cc.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/phrase.cc.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/model.cc.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/quantize.cc.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/read_arpa.cc.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_hashed.cc.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/vocab.cc.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_trie.cc.o\u001b[0m\n",
            "[ 56%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_filter.a\u001b[0m\n",
            "[ 56%] Built target kenlm_filter\n",
            "[ 57%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/sizes.cc.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie.cc.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie_sort.cc.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/value_build.cc.o\u001b[0m\n",
            "[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../bin/probing_hash_table_benchmark\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/virtual_interface.cc.o\u001b[0m\n",
            "[ 63%] Built target probing_hash_table_benchmark\n",
            "[ 65%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/vocab.cc.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/model_buffer.cc.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/print.cc.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/renumber.cc.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/size_option.cc.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm.a\u001b[0m\n",
            "[ 71%] Built target kenlm\n",
            "[ 72%] \u001b[32mBuilding CXX object lm/CMakeFiles/fragment.dir/fragment_main.cc.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object lm/CMakeFiles/query.dir/query_main.cc.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object lm/CMakeFiles/build_binary.dir/build_binary_main.cc.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm_benchmark.dir/kenlm_benchmark_main.cc.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/fragment\u001b[0m\n",
            "[ 77%] Built target fragment\n",
            "[ 78%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/adjust_counts.cc.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../bin/build_binary\u001b[0m\n",
            "[ 80%] Built target build_binary\n",
            "[ 81%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/filter.dir/filter_main.cc.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../bin/query\u001b[0m\n",
            "[ 82%] Built target query\n",
            "[ 83%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/phrase_table_vocab.dir/phrase_table_vocab_main.cc.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/corpus_count.cc.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/phrase_table_vocab\u001b[0m\n",
            "[ 86%] Built target phrase_table_vocab\n",
            "[ 87%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/initial_probabilities.cc.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/interpolate.cc.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/output.cc.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/pipeline.cc.o\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../bin/kenlm_benchmark\u001b[0m\n",
            "[ 92%] Built target kenlm_benchmark\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/filter\u001b[0m\n",
            "[ 93%] Built target filter\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_builder.a\u001b[0m\n",
            "[ 95%] Built target kenlm_builder\n",
            "[ 96%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/count_ngrams.dir/count_ngrams_main.cc.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/lmplz.dir/lmplz_main.cc.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/lmplz\u001b[0m\n",
            "[ 98%] Built target lmplz\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/count_ngrams\u001b[0m\n",
            "[100%] Built target count_ngrams\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Unigram\n",
        "!./build/bin/lmplz -o 1 < /content/train_text.txt > /content/amharic_unigram.arpa\n",
        "\n",
        "# Train Bigram\n",
        "!./build/bin/lmplz -o 2 < /content/train_text.txt > /content/amharic_bigram.arpa\n",
        "\n",
        "\n",
        "# Train Trigram\n",
        "!./build/bin/lmplz -o 3 < /content/train_text.txt > /content/amharic_trigram.arpa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_c6tYsGoHhMi",
        "outputId": "5d58fcb6-9ec0-49c0-e9ba-8c8c9af67e43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1/5 Counting and sorting n-grams ===\n",
            "Reading /content/train_text.txt\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Unigram tokens 117406 types 32529\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:390348\n",
            "Statistics:\n",
            "1 32529 D1=0.686588 D2=1.23098 D3+=1.41702\n",
            "Memory estimate for binary LM:\n",
            "type      kB\n",
            "probing 1397 assuming -p 1.5\n",
            "probing 1524 assuming -r models -p 1.5\n",
            "trie     945 without quantization\n",
            "trie     852 assuming -q 8 -b 8 quantization \n",
            "trie     945 assuming -a 22 array pointer compression\n",
            "trie     852 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "Chain sizes: 1:390348\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:390348\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Name:lmplz\tVmPeak:10779596 kB\tVmRSS:6732 kB\tRSSMax:3565180 kB\tuser:0.336156\tsys:2.45296\tCPU:2.78917\treal:2.80217\n",
            "=== 1/5 Counting and sorting n-grams ===\n",
            "Reading /content/train_text.txt\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Unigram tokens 117406 types 32529\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:390348 2:10886305792\n",
            "Statistics:\n",
            "1 32529 D1=0.716825 D2=1.19714 D3+=1.45515\n",
            "2 88331 D1=0.815018 D2=1.33871 D3+=1.29124\n",
            "Memory estimate for binary LM:\n",
            "type      kB\n",
            "probing 2378 assuming -p 1.5\n",
            "probing 2505 assuming -r models -p 1.5\n",
            "trie    1258 without quantization\n",
            "trie    1011 assuming -q 8 -b 8 quantization \n",
            "trie    1258 assuming -a 22 array pointer compression\n",
            "trie    1011 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "Chain sizes: 1:390348 2:1413296\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:390348 2:1413296\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Name:lmplz\tVmPeak:10787792 kB\tVmRSS:8020 kB\tRSSMax:2923316 kB\tuser:0.369703\tsys:1.48444\tCPU:1.8542\treal:1.85579\n",
            "=== 1/5 Counting and sorting n-grams ===\n",
            "Reading /content/train_text.txt\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Unigram tokens 117406 types 32529\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:390348 2:3786541312 3:7099764736\n",
            "Statistics:\n",
            "1 32529 D1=0.716825 D2=1.19714 D3+=1.45515\n",
            "2 88331 D1=0.852696 D2=1.28765 D3+=1.35081\n",
            "3 93386 D1=0.895401 D2=1.45857 D3+=1.13998\n",
            "Memory estimate for binary LM:\n",
            "type      kB\n",
            "probing 4537 assuming -p 1.5\n",
            "probing 5182 assuming -r models -p 1.5\n",
            "trie    2311 without quantization\n",
            "trie    1545 assuming -q 8 -b 8 quantization \n",
            "trie    2204 assuming -a 22 array pointer compression\n",
            "trie    1438 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "Chain sizes: 1:390348 2:1413296 3:1867720\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:390348 2:1413296 3:1867720\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Name:lmplz\tVmPeak:10779596 kB\tVmRSS:9980 kB\tRSSMax:2479104 kB\tuser:0.425953\tsys:1.28288\tCPU:1.7089\treal:1.68035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load your character‐level labels\n",
        "df = pd.read_csv('/content/char_labels.csv')\n",
        "df_char = df[df.type=='character']\n",
        "\n",
        "# Fit a LabelEncoder on the training split only\n",
        "le = LabelEncoder()\n",
        "le.fit(df_char[df_char.split=='train']['label'])\n",
        "\n",
        "# Your list of characters, in index order 0…num_classes-1\n",
        "char_list = list(le.classes_)"
      ],
      "metadata": {
        "id": "OZbaLaAkQDxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import kenlm\n",
        "import pandas as pd\n",
        "from jiwer import wer\n",
        "from editdistance import eval as edit_distance\n",
        "\n",
        "# ─── 1. Load the trained character CNN ───\n",
        "model = tf.keras.models.load_model('/content/amharic_char_cnn.h5')\n",
        "\n",
        "# ─── 2. Prepare test metadata ───\n",
        "# CSV format: filename,label\n",
        "test_df = pd.read_csv('/content/test_lines.csv')\n",
        "# e.g. test_lines.csv:\n",
        "# line001.png,ሰላም ለዓለም\n",
        "# line002.png,አማርኛ ቋንቋ\n",
        "\n",
        "IMAGE_DIR = '/content/test_line_images'  # folder containing your 48×128 PNGs\n",
        "\n",
        "# ─── 3. Load your KenLM models ───\n",
        "bigram_lm = kenlm.Model('/content/amharic_bigram.arpa')\n",
        "trigram_lm = kenlm.Model('/content/amharic_trigram.arpa')\n",
        "\n",
        "# ─── 4. Build index→char list ───\n",
        "# We need the exact mapping you used during character training:\n",
        "# If you saved `le.classes_`, reload it here:\n",
        "# char_list = list(np.load('char_classes.npy'))\n",
        "# char_classes.npy was saved via: np.save('char_classes.npy', le.classes_)\n",
        "\n",
        "blank_token = ''  # or a special char you reserved\n",
        "\n",
        "# After you load model, char_list, test_df, LM models…\n",
        "\n",
        "def debug_one(fn, gt):\n",
        "    img = cv2.imread(os.path.join(IMAGE_DIR, fn), cv2.IMREAD_GRAYSCALE)\n",
        "    raw = sliding_window_argmax(img, model)\n",
        "    ctc = ctc_collapse(raw)\n",
        "    bi  = lm_rescore(ctc, bigram_lm)\n",
        "    tri = lm_rescore(ctc, trigram_lm)\n",
        "    print(f\"GT  : {gt}\")\n",
        "    print(f\"RAW : {raw}\")\n",
        "    print(f\"CTC : {ctc}\")\n",
        "    print(f\"BI  : {bi}\")\n",
        "    print(f\"TRI : {tri}\")\n",
        "    print(\"-\"*40)\n",
        "\n",
        "# Print first 5\n",
        "for i, row in test_df.head(5).iterrows():\n",
        "    debug_one(row.filename, row.label)\n",
        "\n",
        "\n",
        "# ─── 5. Helper functions ───\n",
        "\n",
        "def sliding_window_argmax(img, model, window_w=32, stride=16):\n",
        "    \"\"\"Stage 1: Raw sliding-window + argmax\"\"\"\n",
        "    preds = []\n",
        "    h, w = img.shape\n",
        "    for x in range(0, w-window_w+1, stride):\n",
        "        patch = img[:, x:x+window_w]\n",
        "        patch = cv2.resize(patch, (32,32)).astype('float32')/255.0\n",
        "        patch = patch[..., None][None,...]   # (1,32,32,1)\n",
        "        prob  = model.predict(patch, verbose=0)[0]  # (num_classes,)\n",
        "        preds.append(char_list[np.argmax(prob)])\n",
        "    return ''.join(preds)\n",
        "\n",
        "def ctc_collapse(seq, blank=blank_token):\n",
        "    \"\"\"Stage 2: merge repeats & drop blanks\"\"\"\n",
        "    out, prev = [], None\n",
        "    for ch in seq:\n",
        "        if ch == prev or ch == blank:\n",
        "            prev = ch\n",
        "            continue\n",
        "        out.append(ch)\n",
        "        prev = ch\n",
        "    return ''.join(out)\n",
        "\n",
        "def lm_rescore(seq, lm_model, alpha=1.0):\n",
        "    \"\"\"\n",
        "    Stage 3/4: naive LM rescoring of one candidate string.\n",
        "    For a full beam search you’d generate N-best, but here\n",
        "    we’ll just return the raw sequence scored by the LM.\n",
        "    \"\"\"\n",
        "    s = seq.replace('  ',' ').strip()\n",
        "    # kenlm score is log10 probability; higher is better\n",
        "    score = lm_model.score(s, bos=True, eos=True)\n",
        "    return s  # placeholder: for real rescoring you'd pick highest-scoring variant\n",
        "\n",
        "def cer(ref, hyp):\n",
        "    \"\"\"Character Error Rate\"\"\"\n",
        "    return edit_distance(ref, hyp) / max(len(ref),1)\n",
        "\n",
        "# ─── 6. Evaluation Loop ───\n",
        "\n",
        "stages = ['raw', 'ctc', 'uni', 'tri']\n",
        "metrics = {st: {'cer':[], 'wer':[]} for st in stages}\n",
        "\n",
        "for _, row in test_df.iterrows():\n",
        "    fn, gt = row.filename, row.label\n",
        "    img = cv2.imread(os.path.join(IMAGE_DIR, fn), cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # 1) RAW\n",
        "    raw_pred = sliding_window_argmax(img, model)\n",
        "    metrics['raw']['cer'].append(cer(gt, raw_pred))\n",
        "    metrics['raw']['wer'].append(wer(gt, raw_pred))\n",
        "\n",
        "    # 2) CTC\n",
        "    ctc_pred = ctc_collapse(raw_pred)\n",
        "    metrics['ctc']['cer'].append(cer(gt, ctc_pred))\n",
        "    metrics['ctc']['wer'].append(wer(gt, ctc_pred))\n",
        "\n",
        "    # 3) Unigram LM\n",
        "    uni_pred = lm_rescore(ctc_pred, bigram_lm)\n",
        "    metrics['uni']['cer'].append(cer(gt, uni_pred))\n",
        "    metrics['uni']['wer'].append(wer(gt, uni_pred))\n",
        "\n",
        "    # 4) Trigram LM\n",
        "    tri_pred = lm_rescore(ctc_pred, trigram_lm)\n",
        "    metrics['tri']['cer'].append(cer(gt, tri_pred))\n",
        "    metrics['tri']['wer'].append(wer(gt, tri_pred))\n",
        "\n",
        "# ─── 7. Print Summary ───\n",
        "for st in stages:\n",
        "    avg_cer = np.mean(metrics[st]['cer'])*100\n",
        "    avg_wer = np.mean(metrics[st]['wer'])*100\n",
        "    print(f\"{st:>3}  CER: {avg_cer:5.2f}%   WER: {avg_wer:5.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgtlplPjFGLL",
        "outputId": "37ee0d9c-06cd-4209-d3f3-50ee01e09be7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GT  : ቀጠሉ፡፡ ጉልላት እሮጦ ሄዶ \n",
            "RAW : ጬጩሡጩሡጬጬ\n",
            "CTC : ጬጩሡጩሡጬ\n",
            "BI  : ጬጩሡጩሡጬ\n",
            "TRI : ጬጩሡጩሡጬ\n",
            "----------------------------------------\n",
            "GT  : ሳይሸራረፉ የሚከበሩበት አግባብ \n",
            "RAW : ጬጬጬጬጩጬጬ\n",
            "CTC : ጬጩጬ\n",
            "BI  : ጬጩጬ\n",
            "TRI : ጬጩጬ\n",
            "----------------------------------------\n",
            "GT  : ለኮምፒዩቲንግ ፋኩልቲ ባህርዳር \n",
            "RAW : ጬጬጬጬጩጬጩ\n",
            "CTC : ጬጩጬጩ\n",
            "BI  : ጬጩጬጩ\n",
            "TRI : ጬጩጬጩ\n",
            "----------------------------------------\n",
            "GT  : ሁፍ የሰፈሩት ሃሳቦች፤ በተዘዋዋሪ \n",
            "RAW : ጬጬጬጬጬጬጬ\n",
            "CTC : ጬ\n",
            "BI  : ጬ\n",
            "TRI : ጬ\n",
            "----------------------------------------\n",
            "GT  : ጉልላት ከተመረቀ በኋላ እግሩ \n",
            "RAW : ጬጬጬጬጩጬጬ\n",
            "CTC : ጬጩጬ\n",
            "BI  : ጬጩጬ\n",
            "TRI : ጬጩጬ\n",
            "----------------------------------------\n",
            "raw  CER: 100.40%   WER: 100.00%\n",
            "ctc  CER: 100.40%   WER: 100.00%\n",
            "uni  CER: 100.40%   WER: 100.00%\n",
            "tri  CER: 100.40%   WER: 100.00%\n"
          ]
        }
      ]
    }
  ]
}